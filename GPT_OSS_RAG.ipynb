{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "SratyzvkVLBo"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain_nvidia_ai_endpoints langchain-community langchain-text-splitters faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import chain"
      ],
      "metadata": {
        "id": "eu7MF7SEVVXV"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')"
      ],
      "metadata": {
        "id": "b0yxOTW8Vr7O"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "AD_C4kyMV0jr"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J70mCG7bcZF_",
        "outputId": "4638ad65-cedf-4cf7-c96e-08c7864ee1c9"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\nGet started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\\nLangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî¬†it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability‚Äã\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\\n\\nGet started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals‚Äã\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering‚Äã\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\\n\\nGet started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n\")]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = NVIDIAEmbeddings()"
      ],
      "metadata": {
        "id": "ez2z6h2QV5Yc"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MZRMkchV9Gb",
        "outputId": "8cf4e784-6aab-4a60-ec15-5325fd92bbf8"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NVIDIAEmbeddings(base_url='https://integrate.api.nvidia.com/v1', model='nvidia/nv-embedqa-e5-v5', truncate='NONE', dimensions=None, max_batch_size=50)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "vector = FAISS.from_documents(documents, embeddings)\n",
        "retriever = vector.as_retriever()"
      ],
      "metadata": {
        "id": "HqPrEscbV95h"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J_R3-nicFv7",
        "outputId": "0611d314-6854-40a3-e7ff-43138ebcc194"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'NVIDIAEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7bcc0a0a5b10>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatNVIDIA(model=\"openai/gpt-oss-20b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-wzociBWAjz",
        "outputId": "824a7412-e8f4-4130-8410-85a3395ded38"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_nvidia_ai_endpoints/_common.py:225: UserWarning: Found openai/gpt-oss-20b in available_models, but type is unknown and inference may fail.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il69x4fZWJjj",
        "outputId": "c740957a-0d0b-4f39-dcf7-f44b17196ee0"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='openai/gpt-oss-20b')"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"tell me more about you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPzUi2rSbnaB",
        "outputId": "9fcfd77b-cb98-4ec1-daa1-b52cc6fd27ae"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={}, response_metadata={'role': 'assistant', 'reasoning_content': 'The user says: \"tell me more about you?\" They want more information about the assistant. So I need to respond with a friendly explanation: I\\'m ChatGPT, a large language model trained by OpenAI. I can answer questions, provide information, etc. I can mention my training data, my capabilities, my limitations, my purpose, etc. I should also be mindful of the policies. There\\'s no policy violation. Just provide a friendly, helpful answer. I should keep it concise but thorough. I can mention that I\\'m not a human, I don\\'t have consciousness, I\\'m a tool, I was trained on a mixture of licensed data, data created by human trainers, and publicly available data. I can answer a wide range of topics. I can mention that I don\\'t have personal experiences, I don\\'t have feelings, I can\\'t access the internet, etc. I can mention that I am a language model. I can mention that I am not perfect, I can sometimes hallucinate. I can mention that I can be used for many tasks: writing, summarizing, etc. I can mention that I can adapt to different tones. I can mention that I can be used as a conversation partner. I can mention that I am safe and follow guidelines. I can mention that I am not a lawyer, doctor, etc. I can mention that I can provide general information but not professional advice. I can mention that I can help with coding, math, etc. I can mention that I can help with creative writing. I can mention that I can help with brainstorming. I can mention that I can help with data analysis. I can mention that I can help with generating prompts. I can mention that I can help with translation. I can mention that I can help with learning new languages. I can mention that I can help with mental health support but not professional therapy. I can mention that I can help with cooking recipes. I can mention that I can help with travel planning. I can mention that I can help with business. I can mention that I can help with academic research. I can mention that I can help with summarizing. I can mention that I can help with writing emails. I can mention that I can help with generating code. I can mention that I can help with debugging. I can mention that I can help with data analysis. I can mention that I can help with generating content. I can mention that I can help with conversation practice. I can mention that I can help with writing prompts. I can mention that I can help with brainstorming. I can mention that I can help with generating ideas. I can mention that I can help with generating stories. I can mention that I can help with writing poems. I can mention that I can help with writing news articles. I can mention that I can help with writing essays. I can mention that I can help with writing research proposals. I can mention that I can help with writing grant proposals. I can mention that I can help with writing CVs. I can mention that I can help with writing cover letters. I can mention that I can help with writing resumes. I can mention that I can help with writing LinkedIn profiles. I can mention that I can help with writing marketing copy. I can mention that I can help with writing product descriptions. I can mention that I can help with writing ad copy. I can mention that I can help with writing social media posts. I can mention that I can help with writing blog posts. I can mention that I can help with writing newsletters. I can mention that I can help with writing press releases. I can mention that I can help with writing scripts. I can mention that I can help with writing video scripts. I can mention that I can help with writing podcasts. I can mention that I can help with writing speeches. I can mention that I can help with writing speeches. I can mention that I can help with writing speeches. I can mention that I can help with writing speeches. (Stop repeating.)\\n\\nWe should keep it concise but thorough. The user asked \"tell me more about you?\" So we should answer with a friendly explanation of what I am, how I work, what I can do, and maybe some limitations. Also mention that I don\\'t have personal experiences or consciousness. We can mention that I can adapt to different tones. We can mention that I can answer many topics. We can mention that I can provide references. We can mention that I can help with a wide range of tasks. We can mention that I can help with learning. We can mention that I can help with debugging. We can mention that I can help with creative writing. We can mention that I can help with summarizing. We can mention that I can help with data analysis. We can mention that I can help with translation. We can mention that I can help with generating code. We can mention that I can help with debugging. We can mention that I can help with generating prompts. We can mention that I can help with brainstorming. We can mention that I can help with writing', 'content': '', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'token_usage': {'prompt_tokens': 73, 'total_tokens': 1097, 'completion_tokens': 1024, 'prompt_tokens_details': None}, 'finish_reason': 'stop', 'model_name': 'openai/gpt-oss-20b'}, id='run--1a936807-2ce5-42c6-a1cf-cd84f41e6fe2-0', usage_metadata={'input_tokens': 73, 'output_tokens': 1024, 'total_tokens': 1097}, role='assistant')"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyde_template = \"\"\"Even if you do not know the full answer, generate a one-paragraph hypothetical answer to the below question:\n",
        "\n",
        "{question}\"\"\""
      ],
      "metadata": {
        "id": "QLA5i3xRXFA2"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyde_prompt = ChatPromptTemplate.from_template(hyde_template)"
      ],
      "metadata": {
        "id": "hgb8kZN0bLx1"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyde_query_transformer = hyde_prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "CJo_f8E8bMmG"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@chain\n",
        "def hyde_retriever(question):\n",
        "    hypothetical_document = hyde_query_transformer.invoke({\"question\": question})\n",
        "    return retriever.invoke(hypothetical_document)"
      ],
      "metadata": {
        "id": "r_f7hwQSbMiu"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3koijhknbVw9"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "4a-WoeKzbWUv"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_chain = prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "W0wZuh0AbWSd"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@chain\n",
        "def final_chain(question):\n",
        "    documents = hyde_retriever.invoke(question)\n",
        "\n",
        "    # FIXED: Check if any document has non-empty page_content\n",
        "    if not documents or not any(getattr(doc, \"page_content\", \"\").strip() for doc in documents):\n",
        "        raise ValueError(\"No valid documents retrieved for the question.\")\n",
        "\n",
        "    for s in answer_chain.stream({\"question\": question, \"context\": documents}):\n",
        "        yield s"
      ],
      "metadata": {
        "id": "jvdPnoGQbbW4"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Retrieved documents:\", documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48VhsTikc3ky",
        "outputId": "5e6c824f-c2d4-44d7-a688-20ce47fa02fd"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved documents: [Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='LangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"LangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî¬†it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability‚Äã\"), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Observability‚Äã\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals‚Äã\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Prompt Engineering‚Äã\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in final_chain.stream(\"how can langsmith help with testing\"):\n",
        "    print(s, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5PJMNbkbb3f",
        "outputId": "8a2bc828-4461-4a4f-ea19-5b5011d2e52c"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangSmith helps you test LLM applications in several ways:\n",
            "\n",
            "* **Evals** – The LangSmith SDK and UI let you build evaluation datasets and metrics, run those tests against your production traffic, score the results, and capture human feedback on the data.  \n",
            "* **Observability** – Tracing, dashboards, and alerts give you real‑time visibility into key metrics (RPS, error rates, cost) so you can spot regressions or failures as soon as they happen.  \n",
            "* **Prompt engineering** – Automatic version control and collaboration features let you iterate on prompts and see how changes affect test outcomes.  \n",
            "\n",
            "Together, these tools give you a complete testing workflow that keeps your model’s quality high and your deployment confidence strong."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "YWxfOOPHdy1L"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_langsmith(question):\n",
        "    return \"\".join(final_chain.stream(question))\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🧠 GPT-OSS RAG\")\n",
        "\n",
        "    with gr.Row():\n",
        "        inp = gr.Textbox(placeholder=\"Ask your question here...\", label=\"Your Question\")\n",
        "    out = gr.Textbox(label=\"Answer\", lines=10)\n",
        "\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    btn.click(fn=ask_langsmith, inputs=inp, outputs=out)"
      ],
      "metadata": {
        "id": "IHwhOvhVl0Ps"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "_vy76vYnl4n6",
        "outputId": "0905fe0a-d0b1-4a40-b7b1-af06b3389c45"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f797a586a559f805ff.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f797a586a559f805ff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "balbv-IKl6xp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}